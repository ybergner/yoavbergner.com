---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Item Response Models for Multiple Attempts With Incomplete Data
subtitle: ''
summary: ''
authors:
- Yoav Bergner
- I. Choi
- K.E. Castellano
tags: []
categories: []
date: '2019-01-01'
lastmod: 2021-04-11T13:52:43-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-04-11T17:52:51.194056Z'
publication_types:
- '2'
abstract: Â© 2019 by the National Council on Measurement in Education Allowance for
  multiple chances to answer constructed response questions is a prevalent feature
  in computer-based homework and exams. We consider the use of item response theory
  in the estimation of item characteristics and student ability when multiple attempts
  are allowed but no explicit penalty is deducted for extra tries. This is common
  practice in online formative assessments, where the number of attempts is often
  unlimited. In these environments, some students may not always answer-until-correct,
  but may rather terminate a response process after one or more incorrect tries. We
  contrast the cases of graded and sequential item response models, both unidimensional
  models which do not explicitly account for factors other than ability. These approaches
  differ not only in terms of log-odds assumptions but, importantly, in terms of handling
  incomplete data. We explore the consequences of model misspecification through a
  simulation study and with four online homework data sets. Our results suggest that
  model selection is insensitive for complete data, but quite sensitive to whether
  missing responses are regarded as informative (of inability) or not (e.g., missing
  at random). Under realistic conditions, a sequential model with similar parametric
  degrees of freedom to a graded model can account for more response patterns and
  outperforms the latter in terms of model fit.
publication: '*Journal of Educational Measurement*'
doi: 10.1111/jedm.12214
---
